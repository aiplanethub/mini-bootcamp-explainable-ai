# The Future of Interpretability

Let us take a look at the possible future of machine learning interpretability.

### The focus will be on model-agnostic interpretability tools

* It is much easier to automate interpretability when it is decoupled(separated) from the underlying machine learning model.
* The advantage of model-agnostic interpretability lies in its modularity, i.e., the degree to which a system's components may be separated and then recombined.
* We can easily replace the underlying machine learning model. We can also just as easily replace the interpretation method.
* For these reasons, model-agnostic methods will scale much better. That is why it is believed that model-agnostic methods will become more dominant in the long term.




### Machine learning will be automated and, with it, interpretability.

* An already visible trend is the automation of model training. That includes automated engineering and selection of features, automated hyperparameter optimization, comparison of different models, and ensembling or stacking of the models. The result is the best possible prediction model.
* Similarly, nobody stops you from automatically computing all these model interpretations. The actual interpretation still requires people.
* Imagine: You upload a dataset, specify the prediction goal and at the push of a button the best prediction model is trained and the program spits out all interpretations of the model.

### Robots and programs will explain themselves

* We need more intuitive interfaces to machines and programs that make heavy use of machine learning.
* Some examples:
  * A self-driving car that reports why it stopped abruptly ("70% probability that a kid will cross the road");
  * A credit default program that explains to a bank employee why a credit application was rejected ("Applicant has too many credit cards and is employed in an unstable job.");
  * A robot arm that explains why it moved the item from the conveyor belt into the trash bin ("The item has a dent at the bottom.").

### Interpretability could boost machine intelligence research

By doing more research on how programs and machines can explain themselves, we can improve our understanding of intelligence and become better at creating intelligent machines.